{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PI4SIUv70TQ3"
      },
      "outputs": [],
      "source": [
        "!pip3 install opencv_python\n",
        "!pip install deepface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from deepface import DeepFace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oc4n4XqL2JNx",
        "outputId": "5476ab88-e7af-4345-de59-0471d185ca0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24-01-24 08:41:18 - Directory /root/.deepface created\n",
            "24-01-24 08:41:18 - Directory /root/.deepface/weights created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyImageProcessor:\n",
        "    #a. constructor\n",
        "  def __init__(self, image_path:str):\n",
        "      self.image_path = image_path\n",
        "      self.img = cv.imread(image_path)\n",
        "      self.face_cascade = cv.CascadeClassifier(cv.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
        "\n",
        "    #b. convert bgr to rgb\n",
        "  def bgr_2_rgb_convertor(self):\n",
        "      img_rgb = cv.cvtColor(self.img, cv.COLOR_BGR2RGB)\n",
        "      plt.imshow(img_rgb)\n",
        "      return img_rgb\n",
        "\n",
        "     #c. convert to gray scale picture and return Numpy array\n",
        "  def bgr_2_gray_scale_convertor(self):\n",
        "      img_gray = cv.cvtColor(self.img, cv.COLOR_BGR2GRAY)\n",
        "      SCALE_FACTOR = 1.1\n",
        "      MINIMUM_NEIGHBORS = 4\n",
        "      faces = self.face_cascade.detectMultiScale(img_gray, SCALE_FACTOR, MINIMUM_NEIGHBORS)\n",
        "      return faces\n",
        "\n",
        "     #d. resize the image of 50%\n",
        "  def _50_percent_resizer(self):\n",
        "      w_ratio = 0.5\n",
        "      h_ratio = 0.5\n",
        "      new_ratio_img = cv.resize(self.img,(0,0),self.img,fx=w_ratio, fy=h_ratio)\n",
        "      plt.imshow(new_ratio_img)\n",
        "      return new_ratio_img\n",
        "\n",
        "\n",
        "      #e.save image into new output_image_path_and_name\n",
        "  def image_writer(self, output_image_path_and_name:str):\n",
        "      cv.imwrite(output_image_path_and_name, self.img)\n",
        "\n",
        "      #f. draw red frame\n",
        "  def frame_it(self,output_image_with_frame_path:str):\n",
        "      rect_color = (255,0,0)\n",
        "      rect_thickness = 4\n",
        "\n",
        "          # Get the detected faces using the detect_faces method\n",
        "      detected_faces = self.detect_faces()\n",
        "\n",
        "      for (x,y,width,height) in detected_faces:\n",
        "          cv.rectangle(self.img,(x,y),(x+width,y+height),\n",
        "                      color = rect_color,\n",
        "                      thickness = rect_thickness)\n",
        "      plt.imshow(self.img)\n",
        "      plt.show()\n",
        "      cv.imwrite(output_image_with_frame_path, self.img)\n",
        "\n",
        "      #g. find image center and draw blue circle\n",
        "  def find_center(self,output_image_with_center):\n",
        "      w_cent = self.img.shape[1]//2\n",
        "      h_cent = self.img.shape[0]//2\n",
        "      cv.circle(img = self.img,\n",
        "                center = (w_cent, h_cent),\n",
        "                radius = 10,\n",
        "                color = (0,0,255),\n",
        "                thickness=-1)\n",
        "      plt.title(\"images center\")\n",
        "      plt.imshow(self.img)\n",
        "      plt.show()\n",
        "\n",
        "      cv.imwrite(output_image_with_center, self.img)\n",
        "\n",
        "      #h. Using CascadeClassifier, and RED rectangles around faces and\n",
        "  def detect_faces(self):\n",
        "      SCALE_FACTOR = 1.1\n",
        "      MINIMUM_NEIGHBORS = 4\n",
        "      rect_color = (255,0,0)\n",
        "      rect_thickness = 3\n",
        "      faces = self.face_cascade.detectMultiScale(self.img, SCALE_FACTOR, MINIMUM_NEIGHBORS)\n",
        "      for (x,y,width,height) in faces:\n",
        "          cv.rectangle(self.img,(x,y),(x+width,y+height),\n",
        "                      color = rect_color,\n",
        "                      thickness = rect_thickness)\n",
        "      # faces = cv.CascadeClassifier(cv.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
        "\n",
        "      print(f\" There are {len(faces)} faces in your image\")\n",
        "      plt.imshow(self.img)\n",
        "      plt.title(\"image with faces detected\")\n",
        "      return faces\n",
        "\n",
        "      #i. detect_faces_and_emotions\n",
        "          #get the image in constructor,\n",
        "          #apply CascadeClassifier to find faces into image\n",
        "          #Get emotions from these faces using DeepFace\n",
        "          #return image\n",
        "            #with greem rectangles around faces\n",
        "            #and text that show the feeling/emotion of the face\n",
        "\n",
        "  def detect_faces_and_emotions(self):\n",
        "      SCALE_FACTOR = 1.1\n",
        "      MINIMUM_NEIGHBORS = 5\n",
        "      faces = self.face_cascade.detectMultiScale(self.img, SCALE_FACTOR, MINIMUM_NEIGHBORS)\n",
        "\n",
        "      # check if any output\n",
        "      if np.any(faces):\n",
        "          print(type(faces))\n",
        "      print(f\"location of our face(s) {faces}\")\n",
        "\n",
        "      # loop over every face we find into pictures\n",
        "      for x, y, width, height in faces:\n",
        "          # Crop the face from the original image\n",
        "          face_img = self.img[y:y + height, x:x + width]\n",
        "\n",
        "          # create instance of Deepface analyze method for each face\n",
        "          emotion = DeepFace.analyze(face_img, actions=\"emotion\")\n",
        "\n",
        "          # log to terminal\n",
        "          print(f\"At frame # {x} face has emotion = {emotion}\")\n",
        "\n",
        "          # create the text of the emotion found over my rectangle of the face we found\n",
        "          emotion_text = f\"{emotion[0]['dominant_emotion']}\"\n",
        "\n",
        "          # write the text emotion over our frame\n",
        "          cv.putText(self.img,  # original BGR picture\n",
        "                      emotion_text,  # Text from emotion object from DeepFace\n",
        "                      (x, y),  # coordinator from face object\n",
        "                      cv.FONT_HERSHEY_SIMPLEX,  # font built-in CV\n",
        "                      0.9,  # alpha that measures text transparency\n",
        "                      color=(0, 0, 255),  # text color\n",
        "                      thickness=2)\n",
        "\n",
        "          # draw rectangle over face detected\n",
        "          cv.rectangle(self.img,  # original picture\n",
        "                        (x, y),  # position for top left corner\n",
        "                        (x + width, y + height),  # position for bottom right corner\n",
        "                        color=(0, 255, 0),\n",
        "                        thickness=2)\n",
        "\n",
        "      plt.imshow(self.img)\n",
        "      plt.show()\n",
        "      return self.img\n",
        "\n"
      ],
      "metadata": {
        "id": "nl4wTRpI2JLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imageprocessor = MyImageProcessor('faces_2.jpg')\n",
        "imageprocessor.bgr_2_rgb_convertor()\n",
        "imageprocessor.bgr_2_gray_scale_convertor()\n",
        "imageprocessor._50_percent_resizer()\n",
        "imageprocessor.image_writer('faces_2.jpg')\n",
        "imageprocessor.frame_it('faces_12.jpg')\n",
        "imageprocessor.find_center('faces_12.jpg')\n",
        "imageprocessor.detect_faces()\n",
        "imageprocessor.detect_faces_and_emotions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "KXmPAHEH7ahP",
        "outputId": "a26fba6f-f348-4d29-ec6f-ea85fc137442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'MyImageProcessor' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-45d1307fdff7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimageprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyImageProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimageprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbgr_2_rgb_convertor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimageprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbgr_2_gray_scale_convertor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimageprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_50_percent_resizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimageprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'faces_2.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MyImageProcessor' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "RiEcwIAW2JIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QfS8G9Rc2JFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rpvyxI_E2I-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VACegFXw2I4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8zmAe6vx2I0l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}